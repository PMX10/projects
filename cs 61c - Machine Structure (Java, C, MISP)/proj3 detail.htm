<html><head><title>CS61C Project 3</title><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw');ol{margin:0;padding:0}.c16{color:#333333;font-size:9pt;background-color:#fff7ce;font-family:Consolas}.c27{color:#333333;font-size:8pt;background-color:#fff7ce;font-family:Consolas}.c14{list-style-type:disc;margin:0;padding:0}.c8{color:#1155cc;background-color:#ffffff;text-decoration:underline}.c28{list-style-type:square;margin:0;padding:0}.c21{list-style-type:decimal;margin:0;padding:0}.c23{list-style-type:circle;margin:0;padding:0}.c2{padding-left:0pt;margin-left:36pt}.c26{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c1{text-align:justify;direction:ltr}.c6{padding-left:0pt;margin-left:72pt}.c3{padding-left:0pt;margin-left:108pt}.c19{color:inherit;text-decoration:inherit}.c11{font-size:10pt;font-family:Consolas}.c0{font-size:12pt;font-weight:bold}.c18{margin-left:36pt}.c15{font-size:18pt}.c4{font-size:12pt}.c12{background-color:#ffffff}.c7{height:11pt}.c17{vertical-align:super}.c24{font-size:14pt}.c25{margin-left:72pt}.c9{direction:ltr}.c30{margin-left:108pt}.c5{font-style:italic}.c20{font-weight:bold}.c29{font-size:16pt}.c22{color:#ff0000}.c13{text-indent:36pt}.c10{text-align:center}.title{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:36pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}.subtitle{padding-top:18pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:24pt;font-family:Georgia;padding-bottom:4pt}p{color:#000000;font-size:11pt;margin:0;font-family:Arial}h1{padding-top:24pt;line-height:1.15;text-align:left;color:#000000;font-size:18pt;font-family:Arial;font-weight:bold;padding-bottom:6pt}h2{padding-top:18pt;line-height:1.15;text-align:left;color:#000000;font-size:14pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h3{padding-top:14pt;line-height:1.15;text-align:left;color:#666666;font-size:12pt;font-family:Arial;font-weight:bold;padding-bottom:4pt}h4{padding-top:12pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:11pt;font-family:Arial;padding-bottom:2pt}h5{padding-top:11pt;line-height:1.15;text-align:left;color:#666666;font-size:10pt;font-family:Arial;font-weight:bold;padding-bottom:2pt}h6{padding-top:10pt;line-height:1.15;text-align:left;color:#666666;font-style:italic;font-size:10pt;font-family:Arial;padding-bottom:2pt}</style></head><body class="c12 c26"><p class="c9 c10"><span class="c15">CS61C Project 3</span></p><p class="c9 c10"><span class="c15">Optimizing Matrix Multiplication</span></p><p class="c7 c9 c10"><span class="c15"></span></p><p class="c9 c10"><span class="c20 c29">Part 1 due Sunday, March 18, 2012 @ 23:59:59</span></p><p class="c9 c10"><span class="c20 c29">Part 2 due Sunday, April 1, 2012 @ 23:59:59</span></p><p class="c7 c9 c10"><span class="c15 c20"></span></p><p class="c9 c10"><span class="c15">TA: Rimas Avizienis</span></p><p class="c1 c7"><span class="c24 c22"></span></p><p class="c1"><span class="c15 c20">Summary</span></p><p class="c1"><span class="c4">In this project, you will implement an optimized single-precision (32-bit) floating point matrix-matrix multiplication (MMM) routine for square (N x N) matrices. &nbsp;In Part 1, you will use SIMD instructions to implement a MMM kernel tuned for performance on small matrices. &nbsp;In Part 2, you will apply cache blocking to your MMM kernel and tune it to perform well on larger matrices. &nbsp; Finally, you will parallelize your code to run on multiple cores and see how well its performance scales.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">For each part of the project, you will submit source code and a writeup through your class github repository. &nbsp;Part of your grade for each part will be based on your writeup, in which you will briefly describe your implementation and analyze its performance. The other part will be based on our evaluation of your code&rsquo;s performance and correctness.</span></p><p class="c1 c7 c13"><span class="c4"></span></p><p class="c1"><span class="c15 c20">Background</span></p><p class="c1"><span class="c4">Matrix multiplication is a fundamental linear algebra operation that is at the core of many important numerical algorithms. &nbsp;As such, performing MMM efficiently is of interest to &nbsp;people working in a wide range of domains. &nbsp; To refresh your memory, if A,B, and C are N x N matrices, then C=AB is also an N x N matrix, and the value of each element in C is defined as:</span></p><p class="c1 c13 c30"><img src="images/image00.png"></p><p class="c1"><span class="c4">The subscripts denote the position (row and column) of a particular entry in a matrix. &nbsp;For this project, your code should implement the computation </span><span class="c0">C=C+AB</span><span class="c4">. &nbsp;</span></p><p class="c1 c13"><span class="c4">For the purposes of this project, matrices will be stored in memory in </span><span class="c0">column-major order</span><span class="c4">. &nbsp;This means that &nbsp;the element in row i, column j of a matrix will be offset by </span><span class="c0">i+j*N</span><span class="c4">&nbsp; entries relative to the first element of the matrix. &nbsp;Each entry in C is the cross product of a row of A and a column of B, and requires N multiplications and additions to compute. It takes a total of 2N</span><span class="c4 c17">3</span><span class="c4">&nbsp;floating point operations to compute C. Each matrix occupies 4N</span><span class="c4 c17">2</span><span class="c4">&nbsp;bytes of memory, and every element in the A and B matrices is reused N</span><span class="c4 c17">2</span><span class="c4">&nbsp;times throughout the course of the computation.</span></p><p class="c1 c13"><span class="c4">As you have seen in labs 7 &amp; 8, the simplest way to implement MMM is by using three nested loops. &nbsp;To significantly improve upon the performance of the naive MMM implementation, your code will need to do two things:</span></p><p class="c1 c7"><span class="c4"></span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c20 c24">Use the X86 instruction set&rsquo;s SIMD (SSE) instructions</span></li></ol><p class="c1"><span class="c4">To take full advantage of the compute resources available in modern x86 CPUs, your code must use SSE instructions to perform multiple floating point operations in parallel whenever possible. &nbsp;Recall that SSE registers are 128-bits wide, so a single SSE instruction can compute four single-precision floating point results at once. &nbsp;</span><span class="c4">Each core has 16 SSE registers (named xmm0, xmm1, etc) and (under ideal circumstances) can issue both an SSE multiply and add instruction during a single cycle. &nbsp; The hive machines have two 2.4GHz Intel Xeon E5620 4 core CPUs - one in each of the two sockets on the motherboard. &nbsp;We can use this information to calculate the theoretical peak single-precision floating point performance of these machines: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c1 c18 c13"><span class="c0">2</span><span class="c4">&nbsp;instructions (1 multiplication, 1 addition) per cycle </span><span class="c0">*</span></p><p class="c1 c13 c18"><span class="c0">4</span><span class="c4">&nbsp;single precision floating point operations per instruction (flops) </span><span class="c0">*</span></p><p class="c1 c18 c13"><span class="c0">2.4x109</span><span class="c4">&nbsp;cycles per second = </span></p><p class="c1 c18 c13"><span class="c0">19.2 Gflop/s</span><span class="c4">&nbsp;per core (19.2 giga flops) * </span><span class="c0">8 cores</span><span class="c4">&nbsp;=</span></p><p class="c1 c18 c13"><span class="c0">153.6 GFlop/s</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">In practice, performance can be limited either by the available compute resources (CPU bound), or by the rate at which the CPU can access memory (memory bound). Which factor ultimately limits the performance of a particular implementation of an algorithm depends on its arithmetic intensity (the ratio of arithmetic operations performed to bytes of memory accessed) and the target platform&rsquo;s compute resources and memory hierarchy. &nbsp; For more on this topic, see the discussion of the &ldquo;roofline model&rdquo; in the textbook (look it up in the index) and the second paper in the references section of this document. &nbsp;</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Parallelizing an algorithm to run on multiple cores incurs overheads related to communication and synchronization. &nbsp;As a result, performance for parallel codes rarely scales linearly as the number of cores increases beyond a certain point.</span></p><p class="c1 c7 c13"><span class="c4"></span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c24 c20">Use blocking to improve cache performance</span></li></ol><p class="c1"><span class="c4">As you saw in Lab 7, the order in which a program accesses memory can have a drastic impact on how long it takes to run. &nbsp;Your MMM code should use at least two levels of blocking (for the cache and SSE registers) to obtain good performance from the memory hierarchy. &nbsp;For an in-depth discussion of blocking strategies for MMM, take a look at the first paper listed in the references section. &nbsp;The processors you&rsquo;ll be using have 3 levels of caches. &nbsp;Each of the four cores in a socket has private L1 instruction &amp; data caches (32KB, 8 way set associative) and a private unified L2 cache (256KB, 8 way set associative). &nbsp;The outermost level of the memory hierarchy consists of a 12MB L3 cache shared by all four cores in a socket, and three memory controllers that communicate with external banks of DRAM.</span></p><p class="c1 c7 c13"><span class="c4"></span></p><p class="c1"><span class="c15 c20">Getting Started</span></p><p class="c1"><span class="c4">Copy the project materials to a directory in your github tree:</span></p><p class="c1"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cp &ndash;R ~cs61c/proj/sp12_03 proj3</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">We provide you with the following files:</span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c4 c5">benchmark.c</span><span class="c4">&nbsp;&ndash; a &ldquo;test harness&rdquo; which measures the performance and verifies the output &nbsp;of your MMM &nbsp;implementations.</span></li><li class="c2 c1"><span class="c4 c5">sgemm-naive.c</span><span class="c4">&nbsp;&ndash; a naive, 3 nested loop implementation of MMM</span></li><li class="c2 c1"><span class="c4 c5">&nbsp;Makefile</span></li></ol><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">To build the naive MMM implementation, run &ldquo;make bench-naive&rdquo; in your project directory. &nbsp;Running &ldquo;.</span><span class="c4">/ben</span><span class="c4">ch-naive&rdquo; will measure its performance on a set of input sizes from the range 64 &lt;= N &lt;= 1024. &nbsp;Place your register blocked MMM kernel implementation in a file called sgemm-small.c. &nbsp;You can create a template to start from by making a copy of sgemm-naive.c </span><span class="c4">&nbsp;</span><span class="c4">(cp sgemm-naive.c sgemm-small.c). &nbsp;Be sure to #include the necessary header files directives before trying to call any SSE intrinsic functions. Running &ldquo;make bench-small&rdquo; and then running &ldquo;./bench-small&rdquo; will benchmark and verify the MMM kernel in sgemm-small.c.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">You do not need to modify or submit any of the provided files (except possibly the Makefile, if you choose to). &nbsp;Each part of the project will require you to submit several new files:</span></p><p class="c1"><span class="c4">&nbsp;</span></p><p class="c1"><span class="c0">&nbsp;Part 1: </span><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c4 c5">sgemm-small.c &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;part1.pdf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Makefile (optional)</span></p><p class="c1"><span class="c0">&nbsp;Part 2: &nbsp;</span><span class="c4">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c4 c5">sgemm-all.c &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sgemm-openmp.c &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;part2.pdf &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Makefile (optional)</span></p><p class="c1"><span class="c4">&nbsp;</span></p><p class="c1"><span class="c4">You are not allowed to show your code to other students (except your partner), TAs, professors, or anyone else. &nbsp;Copying code you find on the internet is also strictly prohibited and will result in a failing grade on the project and other dire consequences.</span></p><p class="c1"><span class="c4">&nbsp;</span></p><p class="c1"><span class="c15 c20">Assignment</span></p><p class="c1"><span class="c0">&nbsp;</span></p><p class="c1"><span class="c24 c20">Part 1 (due 3/17/2012 @ 23:59:59) (40 points)</span></p><p class="c1"><span class="c0">&nbsp;</span></p><p class="c1"><span class="c4">Implement a single-precision MMM kernel for square matrices that is optimized for performance on inputs with N=64. &nbsp;Here is a suggested order in which to apply various optimizations:</span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c0">Use SSE Instructions</span></li></ol><p class="c1 c13"><span class="c4">Your code will need to use SSE instructions to get good performance on the processors you are using. &nbsp;Your code should definitely use the </span><span class="c4 c5">_mm_add_ps, _mm_mul_ps, _mm_loadu_ps</span><span class="c4">&nbsp;intrinsics as well as some subset of:</span></p><p class="c1 c13"><span class="c4 c5">_mm_load1_ps, _mm_load_ss, _mm_storeu_ps, _mm_store_ss</span></p><p class="c1 c13"><span class="c4 c5">_mm_shuffle_ps, _mm_hadd_ps</span></p><p class="c1 c13"><span class="c4">Depending on how you choose to arrange data in registers and structure your computation, you may not need to use all of these intrinsics (such as </span><span class="c4 c5">_mm_hadd_ps</span><span class="c4">&nbsp;or </span><span class="c4 c5">_mm_shuffle_ps</span><span class="c4">). &nbsp;There are multiple ways to implement MMM using SSE instructions that perform acceptably well. &nbsp;You probably don&rsquo;t want to use some of the newer SSE instructions, such as those that calculate dot products (though you are welcome to try!). &nbsp;You will need to figure out how to handle matrices for which N isn&rsquo;t divisible by 4 (the SSE register width in 32-bit floats).</span></p><p class="c1 c13"><span class="c4">You should always use unaligned loads and stores (</span><span class="c4 c5">_mm_loadu_ps</span><span class="c4">&nbsp;and </span><span class="c4 c5">_mm_storeu_ps</span><span class="c4">). &nbsp;On the processors you are using for this project, these instructions perform as fast as their aligned counterparts (_mm_load_ps and _mm_store_ps) when operating on aligned addresses. &nbsp;The only reason you might want to use the aligned versions is to see if you are accessing memory using aligned addresses (your program will die with a &ldquo;segmentation fault&rdquo; error if you are). &nbsp;Accessing memory aligned on 128-bit boundaries is significantly faster, so you should do it when you can.</span></p><p class="c1 c7 c13"><span class="c4"></span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c0">Re-use values in registers (&ldquo;register blocking&rdquo;)</span></li></ol><p class="c1 c13"><span class="c4">Your code should re-use values once they have been loaded into registers as much as possible. &nbsp;By reusing values loaded from the A or B matrices in multiple calculations, you can reduce the total number of times each value is loaded from memory in the course of computing all the entries in C. &nbsp;To ensure that a value gets loaded into a register and reused instead of being loaded from memory repeatedly, you should assign it to a local variable and refer to it using that variable.</span></p><p class="c1 c7 c13"><span class="c4"></span></p><ol class="c14" start="1"><li class="c1 c2"><span class="c0">Loop unrolling and instruction scheduling</span></li></ol><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unroll the inner loop of your code to improve your utilization of the pipelined SSE multiplier and adder units, and also reduce the overhead of address calculation and loop condition checking. &nbsp;</span><span class="c4">You can reduce the amount of address calculation necessary in the inner loop of your code by accessing memory using constant offsets, whenever possible.</span></p><p class="c1 c13"><span class="c4">The processors you are using can issue a load, a multiply and an add instruction during a single cycle (assuming all 3 are ready to execute and appear reasonably close to one another in the instruction stream). &nbsp;For your enlightenment, here are the latencies (how many cycles it takes to produce a result) for the functional units you&rsquo;ll be using:</span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SSE loads (that hit in the L1 Cache) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 cycles</span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SSE single precision adds and multiplies&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 cycles</span></p><p class="c1 c13"><span class="c4">You can experiment with changing the ordering of instructions (i.e. interleaving independent loads, stores and multiplies) to try and maximize your utilization of the processor. &nbsp;Keep in mind that the compiler is trying to do this for you to some extent, so depending on the particulars of your code, this may or may not have much of an effect. &nbsp;You can look at the compiler generated assembly code to see what effect your C source code manipulations have had.<br></span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c0">Memory access</span></li></ol><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It&rsquo;s generally a good idea to access memory in sequential chunks whenever possible. &nbsp; You may find that it is possible to achieve better performance by including a &ldquo;preprocessing&rdquo; step where you transpose one of the matrices into a local buffer, so you can access both the A and B matrices using &nbsp;sequential addresses. &nbsp;To handle matrix sizes that aren&rsquo;t a multiple of 4, you can try copying them into larger local buffers and padding the edges with zeros. &nbsp;This could allow you to reuse your already optimized SSE code. &nbsp;Another reason you may want to try copying data into a local buffer is so that you can guarantee that all memory accesses will be aligned.</span></p><p class="c1"><span class="c4">&nbsp;</span></p><p class="c1"><span class="c24 c20">Grading</span></p><p class="c1 c7"><span class="c24"></span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Your grade for part 1 will depend on 3 things:</span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c0">Correctness&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(10 points)</span></li></ol><p class="c1 c18"><span class="c4">Your code must correctly compute the output for input matrices with N between 64 and 1024. &nbsp;To get these points, your code must implement at least some of the above mentioned optimizations. &nbsp;You can&rsquo;t just submit the same naive code we gave you and expect to get these points.</span></p><ol class="c14" start="2"><li class="c2 c1"><span class="c0">Performance&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(20 points)</span></li></ol><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To get all available points, your code must achieve at least </span><span class="c0">10 GFlops/s</span><span class="c4">&nbsp;of performance on 64x64 input matrices, as measured on the hive machines. &nbsp;For reference, a highly tuned library implementation of MMM (from GotoBLAS) achieves ~14.5 GFlops/s for such matrices on the same machines. &nbsp;We will compile your code using the Makefile we provided, unless you provide your own. &nbsp;You are welcome to try experimenting with different compiler optimization flags, however we highly recommend that you not use </span><span class="c0">-O3</span><span class="c4">&nbsp;while developing your code, as it often produces slower code than </span><span class="c0">-O2</span><span class="c4">. &nbsp;You may not use any OpenMP directives in your submission for part 1.</span></p><ol class="c14" start="1"><li class="c2 c1"><span class="c0">Writeup&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(10 points)</span></li></ol><p class="c1 c13"><span class="c4">In addition to your code, submit a 1-2 page report (in PDF form) called &ldquo;part1.pdf.&rdquo; &nbsp;Your writeup should:</span></p><ol class="c23" start="1"><li class="c1 c6"><span class="c4">Describe how you use SSE registers in the innermost loop of your code (max 200 words)</span></li><li class="c1 c6"><span class="c4">Describe how you deal with the fringes of matrices when N isn&rsquo;t evenly divisible by the SSE register width (max 100 words)</span></li><li class="c1 c6"><span class="c4">Include a plot showing the performance of your code as compared to &nbsp;the code in sgemm-naive.c</span></li><li class="c1 c6"><span class="c4">Look at the compiler generated X86 assembly code and answer the following questions:</span></li></ol><ol class="c28" start="1"><li class="c1 c3"><span class="c4">How many XMM registers does your code use? </span></li><li class="c1 c3"><span class="c4">Are any values being spilled to the stack during the innermost loop?</span></li><li class="c1 c3"><span class="c4">How many scalar floating point instructions does your code use and why?</span></li></ol><p class="c1 c25"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c1"><span class="c4">Once you have finished Part 1, push your submission (two files: part1.pdf and sgemm-small.c) to the proj3/ directory of your class account github repo, and tag the commit with the label &ldquo;proj3-1.&rdquo; &nbsp;If you worked with a partner, </span><span class="c0">make sure both your names and class accounts appear at the top of both files (writeup and source code)</span><span class="c4">. &nbsp;Failure to do so will cost you points. &nbsp;If you require a TA&rsquo;s help to submit your project through github, and the TA determines that you could have dealt with the situation on your own, you will be penalized. &nbsp;Likewise, if you tag the wrong commit you will also be subject to a penalty. &nbsp;We sincerely hope that you&rsquo;ve figured out how git works by now - if you still find it confusing, please refer to one of the many online resources on the subject. &nbsp;Remember that you can check which files are present in a commit that a tag points to through github&rsquo;s web interface. &nbsp;Don&rsquo;t worry if the commit includes more than just the proj3/ directory - we will only look in that directory for files related to this project.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c0">Part 2 (due 4/1/2012 @ 23:59:59) (40 points)</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Using the MMM kernel you wrote for part 1 as a starting point, add one or more levels of cache blocking to improve its performance for larger matrix sizes. &nbsp;Measure how your code performs on inputs with odd sizes (e.g. 65x65, 127x127, etc.) and optimize your fringe handling if necessary. &nbsp;To improve performance on large matrices, you may want to experiment with copying sub-blocks of your input matrix into contiguous regions of memory. &nbsp;Put your improved code into a file called &ldquo;sgemm-all.c&rdquo; &nbsp;</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Once your serial implementation performs to your satisfaction, use at least one OpenMP #pragma to parallellize it. &nbsp; This should be fairly straight-forward. &nbsp;To improve the performance and scalability of your parallelized MMM code, you should try reordering loops and playing with the &ldquo;schedule&rdquo; directive. &nbsp;Place your parallelized code in a file called &ldquo;sgemm-openmp.c&rdquo;. &nbsp;</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c0">Batch Queueing System</span></p><p class="c1 c7"><span class="c0"></span></p><p class="c1"><span class="c4">To aid you in measuring the performance of your parallelized code, we have set up a batch queueing system on the hive cluster. &nbsp;One machine (hive1) has been designated as the &ldquo;head&rdquo; node, and four machines (hive25-28) have been configured as &ldquo;compute&rdquo; nodes. &nbsp;The compute nodes have been set up such that remote logins are not permitted, so jobs that are run on those machines are guaranteed to be free of interference from other processes running on the machines.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">To submit a job to run on one of the compute servers, you will need to make a shell script that looks like this:</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c16">#!/bin/bash</span></p><p class="c9 c13"><span class="c27">#PBS</span><span class="c16">&nbsp;-N CS61C</span></p><p class="c9 c13"><span class="c27">#PBS</span><span class="c16">&nbsp;-V</span></p><p class="c9 c13"><span class="c27">#PBS</span><span class="c16">&nbsp;-l nodes=1</span></p><p class="c9 c13"><span class="c27">#PBS</span><span class="c16">&nbsp;-q batch</span></p><p class="c9 c13"><span class="c16">cd $PBS_O_WORKDIR</span></p><p class="c9 c13"><span class="c16"># workaround to fix a thread affinity problem in Linux</span></p><p class="c9 c13"><span class="c16">export GOTOBLAS_MAIN_FREE=1</span></p><p class="c9 c13"><span class="c16"># name of the file to execute</span></p><p class="c1 c13"><span class="c16">./bench-openmp</span></p><p class="c1 c7 c13"><span class="c4"></span></p><p class="c1"><span class="c4">Put the script in your working directory (the same one that has the bench-openmp executable that you&rsquo;d like to test) called openmp-test.sh. &nbsp;To submit your job to the batch queue system, execute the following command.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;qsub openmp-test.sh</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">To check the status of jobs currently in the batch queue system, use the qstat command:</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c11">Job id Name User Time Use S Queue</span></p><p class="c1"><span class="c11">------------------------- ---------------- --------------- -------- - -----</span></p><p class="c1"><span class="c11">25.hive1 CS61C cs61c-ta 0 R batch </span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">the &quot;R&quot; indicates that the job is running. &quot;C&quot; means completed, and &quot;Q&quot; means queued (if all 4 compute nodes are busy).</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Calling qstat with the &quot;-n&quot; option will show you which node a task is running on:</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c11">Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time</span></p><p class="c1"><span class="c11">-------------------- -------- -------- ---------------- ------ ----- --- ------ -----</span></p><p class="c1"><span class="c11">25.hive1.cs.berk cs61c-ta batch CS61C 6891 1 1 -- 01:00 C 00:00</span></p><p class="c1"><span class="c11">hive25/0</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">which shows you that my job ran on hive25. The output from your job will be in a file called CS61c.o&lt;num&gt; where &lt;num&gt; is replaced by the Job ID.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">If you have any questions about the batch queue system, please post them on piazza. &nbsp;</span></p><p class="c1 c7"><span class="c0"></span></p><p class="c1"><span class="c0">NOTE</span><span class="c4">: &nbsp;When measuring the performance of your sgemm-openmp code, we will run it on an unloaded hive machine but we will </span><span class="c0">not</span><span class="c4">&nbsp;use the batch queue system. &nbsp;</span></p><p class="c1 c7"><span class="c0 c22"></span></p><p class="c1"><span class="c24 c20">Grading</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Your grade for part 2 will depend your code&rsquo;s performance and a writeup.</span></p><ol class="c14" start="3"><li class="c2 c1"><span class="c0">Performance&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(30 points)</span></li></ol><p class="c1 c13"><span class="c4">Half of the performance component of your grade for part 2 will be based on the performance of your sgemm-all code, and the other half will be based on the performance of your sgemm-openmp code. &nbsp; In both cases, we will be measuring the </span><span class="c0">average</span><span class="c4">&nbsp;performance of your code over a range of matrix sizes.</span></p><p class="c1 c13"><span class="c4">For your sgemm-all code, we will take the average over all the matrix sizes included in the test set in benchmark.c. &nbsp;An average of </span><span class="c0">10 GFlop/s</span><span class="c4">&nbsp;will earn you 80% of the available points, and an average of </span><span class="c0">12 GFlop/s</span><span class="c4">&nbsp;will earn you 100% of the available points.</span></p><p class="c1 c13"><span class="c4">We will only consider the performance of your openmp code for matrices where N is between </span><span class="c0">512</span><span class="c4">&nbsp;and </span><span class="c0">1024</span><span class="c4">. &nbsp;Average performance of </span><span class="c0">40 GFlop/s </span><span class="c4">will earn you 80% of the available points, and an average over </span><span class="c0">50 GFlop/s </span><span class="c4">will earn you 100% of the available points.</span></p><ol class="c14" start="2"><li class="c2 c1"><span class="c0">Writeup&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(10 points)</span></li></ol><p class="c1 c13"><span class="c4">Your writeup for part 2 should include:</span></p><ol class="c23" start="5"><li class="c1 c6"><span class="c4">A brief description of any changes you made to your code from part 1 in order to get it to run well on a range of matrix sizes (max 150 words)</span></li><li class="c1 c6"><span class="c4">A brief description of how you used OpenMP pragmas to parallelize your code in sgemm-openmp.c (max 150 words).</span></li><li class="c1 c6"><span class="c4">A plot showing the speedup of sgemm-all.c over sgemm-naive.c for values of N between 64 and 1024</span></li><li class="c1 c6"><span class="c4">A weak scaling plot of the performance of your sgemm-openmp.c code (use your sgemm-all.c code as the baseline for the single threaded case)</span></li><li class="c1 c6"><span class="c4">A strong scaling plot of the performance of your sgemm-openmp.c code</span></li></ol><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">Once you have completed part 2, push the required files to the proj3/ directory of your class account github repo, and tag the commit with the label &ldquo;proj3-2&rdquo;. &nbsp;</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c0">Extra Credit (due 4/22/2012 @ 23:59:59)<br></span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We will be having a class competition to determine which person or team can code the fastest serial and parallel implementations of MMM for square matrices. &nbsp;Submissions are due Sunday, April 22 at midnight and must be submitted through github. &nbsp;</span><span class="c4 c22">Do not just resubmit your part 2 submission to the extra credit contest - you must implement at least one non-trivial optimization. &nbsp;</span><span class="c4">Winners will be announced in class on Tuesday, April 24th. &nbsp;The top 3 teams in each category (serial and parallel) will be awarded extra credit. &nbsp;Results within 100 MFlop/s will be considered tied, and both submissions will be awarded extra credit. &nbsp;Performance for both the serial and parallel codes will be measured on a randomly selected subset of matrix sizes within the range 512 &lt; N &lt; 1024 (not the same subset used in part 2 of the assignment).</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c0">Submission:</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1 c13"><span class="c4">Create a proj3-ec/ directory in your class github repository that contains the following files:</span></p><p class="c1 c7 c13"><span class="c4"></span></p><ol class="c23" start="1"><li class="c1 c6"><span class="c4">sgemm-ec-serial.c (serial implementation)</span></li><li class="c1 c6"><span class="c4">sgemm-ec-parallel.c (parallel implementation)</span></li><li class="c1 c6"><span class="c4">report-ec.pdf (writeup)</span></li></ol><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tag the commit with the tag </span><span class="c0">proj3-ec</span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c1"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You don&rsquo;t need to submit both a serial and a parallel implementation, but you are welcome to. &nbsp;Your writeup should include performance graphs and a description of optimizations you attempted. &nbsp;If your submission includes a serial implementation, plot the performance of your extra credit submission (sgemm-ec-serial.c) against your part 2 submission (sgemm-all.c). &nbsp;For submissions that include a parallel implementation, include strong and weak scaling plots for your part 2 code (sgemm-openmp.c) and your extra credit submission (sgemm-ec-parallel.c) on the same plot (two graphs total - two lines per graph). &nbsp;For both serial and parallel implementations, Include a list of optimizations you attempted and summarize the effectiveness of each one in a few sentences.</span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1"><span class="c15 c20">References</span></p><p class="c1 c7"><span class="c4"></span></p><ol class="c21" start="1"><li class="c2 c9"><span class="c12">Goto, K., and van de Geijn, R. A. 2008.</span><span class="c8"><a class="c19" href="http://portal.acm.org/citation.cfm?id=1356053">&nbsp;Anatomy of High-Performance Matrix Multiplication</a></span><span class="c12">, </span><span class="c12 c5">ACM Transactions on Mathematical Software 34</span><span class="c12">, 3, Article 12.</span></li><li class="c2 c9"><span class="c12">Chellappa, S., Franchetti, F., and P&uuml;schel, M. 2008.</span><span class="c8"><a class="c19" href="http://www.google.com/url?q=http%3A%2F%2Fspiral.ece.cmu.edu%3A8080%2Fpub-spiral%2Fabstract.jsp%3Fid%3D100&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNF78ztoHz6K4vBo4yTSfg4G5-wEcg">&nbsp;How To Write Fast Numerical Code: A Small Introduction</a></span><span class="c12">, </span><span class="c12 c5">Lecture Notes in Computer Science 5235</span><span class="c12">, 196&ndash;259.</span></li><li class="c2 c9"><span class="c12">Bilmes, </span><span class="c12 c5">et al.</span><span class="c8"><a class="c19" href="http://www.icsi.berkeley.edu/%7Ebilmes/phipac/">&nbsp;The PHiPAC (Portable High Performance ANSI C) Page for BLAS3 Compatible Fast Matrix Matrix Multiply</a></span><span class="c12">.</span></li><li class="c2 c9"><span class="c12">Lam, M. S., Rothberg, E. E, and Wolf, M. E. 1991.</span><span class="c8"><a class="c19" href="http://suif.stanford.edu/papers/lam-asplos91.pdf">&nbsp;The Cache Performance and Optimization of Blocked Algorithms</a></span><span class="c12">, </span><span class="c12 c5">ASPLOS&#39;91</span><span class="c12">, 63&ndash;74.</span></li><li class="c2 c9"><span class="c12">Intel Instrinsics Guide (see Lab 8)</span></li><li class="c2 c9"><span class="c8"><a class="c19" href="http://www.intel.com/Assets/PDF/manual/248966.pdf">Intel&reg; 64 and IA-32 Architectures Optimization Reference Manual</a></span></li><li class="c2 c9"><span class="c8"><a class="c19" href="http://www.plutospin.com/files/OpenMP_reference.pdf">OpenMP reference sheet</a></span></li><li class="c2 c9"><span class="c8"><a class="c19" href="https://computing.llnl.gov/tutorials/openMP">OpenMP Reference Page from LLNL</a></span></li></ol><p class="c1 c7"><span class="c4"></span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1 c7"><span class="c4"></span></p><p class="c1 c7"><span class="c4"></span></p></body></html>